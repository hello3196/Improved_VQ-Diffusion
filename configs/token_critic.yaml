transformer_config:
  target: image_synthesis.modeling.transformers.token_critic.Text2Logit
  params:
    attn_type: 'selfcross'
    n_layer: 16 #19
    condition_seq_len: 77    ###### 77 for clip and 256 for dalle
    content_seq_len: 1024  # 32 x 32
    content_spatial_size: [32, 32]
    n_embd: 1024 # the dim of embedding dims
    condition_dim: 512
    n_head: 16 #16
    attn_pdrop: 0.0
    resid_pdrop: 0.0
    block_activate: GELU2
    timestep_type: 'adalayernorm'    # adainsnorm or adalayernorm and abs
    mlp_hidden_times: 4

condition_emb_config:
  target: image_synthesis.modeling.embeddings.clip_text_embedding.CLIPTextEmbedding
  params:
    clip_name: 'ViT-B/32'
    num_embed: 49408 # 49152+256
    normalize: True
    pick_last_embedding: False   # if True same as clip but we need embedding of each word
    keep_seq_len_dim: False
    additional_last_embedding: False
    embed_dim: 512

content_emb_config:
  target: image_synthesis.modeling.embeddings.dalle_mask_image_embedding.DalleMaskImageEmbedding
  params:
    num_embed: 2887
    spatial_size: !!python/tuple [32, 32]
    embed_dim: 1024
    trainable: True
    pos_emb_type: embedding

condition_codec_config:
  target: image_synthesis.modeling.codecs.text_codec.tokenize.Tokenize
  params:
    context_length: 77     ############# 77 for clip and 256 for dalle
    add_start_and_end: True
    with_mask: True
    pad_value: 0 # 0 for clip embedding and -100 for others
    clip_embedding: False     ############################   if we use clip embedding 
    tokenizer_config:
      target: image_synthesis.modeling.modules.clip.simple_tokenizer.SimpleTokenizer   #########
      params:
        end_idx: 49152                              ###################